# @package model_kwargs
model_type: small_lm
model_config:
  vocab_size: 4
  hidden_size: 64
  num_hidden_layers: 1
  num_attention_heads: 1
  intermediate_size: 256
  max_position_embeddings: 128
  use_parallel_residual: False
  attention_bias: False
  use_cache: False
  is_decoder: True
  classifier_dropout: 0.0