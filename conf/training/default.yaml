# @package training_kwargs

train_steps: 50000
batch_size: 64
learning_rate: 0.0005
context_length: 128
warmup_steps: 0
lr_scheduler_type: constant
lr_scheduler_kwargs: {}
gradient_accumulation_steps: 1
max_grad_norm: 1.0
weight_decay: 0.0
optimizer: adamw_torch
save_online_eval: false
logging_steps: 100
eval_steps: 100
pre_batch_data_collator: true # if batching data in getitem function - usually much faster
save_steps_method: end_only
save_steps: null # if null, the save_steps_method will be used to determine the save_steps